import os
import datetime
from zipfile import ZipFile

import numpy as np
from dask.delayed import Delayed
from geokube.core.datacube import DataCube
from geokube.core.dataset import Dataset

from geodds_utils.queries import GeoQuery

from messaging import Message


def get_file_name_for_climate_downscaled(kube: DataCube, message: Message):
    query: GeoQuery = GeoQuery.parse(message.content)
    is_time_range = False
    if query.time:
        is_time_range = "start" in query.time or "stop" in query.time
    var_names = list(kube.fields.keys())
    if len(kube) == 1:
        if is_time_range:
            FILENAME_TEMPLATE = "{ncvar_name}_VHR-PRO_IT2km_CMCC-CM_{product_id}_CCLM5-0-9_1hr_{start_date}_{end_date}_{request_id}"
            ncvar_name = kube.fields[var_names[0]].ncvar
            return FILENAME_TEMPLATE.format(
                product_id=message.product_id,
                request_id=message.request_id,
                ncvar_name=ncvar_name,
                start_date=np.datetime_as_string(
                    kube.time.values[0], unit="D"
                ),
                end_date=np.datetime_as_string(kube.time.values[-1], unit="D"),
            )
        else:
            FILENAME_TEMPLATE = "{ncvar_name}_VHR-PRO_IT2km_CMCC-CM_{product_id}_CCLM5-0-9_1hr_{request_id}"
            ncvar_name = kube.fields[var_names[0]].ncvar
            return FILENAME_TEMPLATE.format(
                product_id=message.product_id,
                request_id=message.request_id,
                ncvar_name=ncvar_name,
            )
    else:
        if is_time_range:
            FILENAME_TEMPLATE = "VHR-PRO_IT2km_CMCC-CM_{product_id}_CCLM5-0-9_1hr_{start_date}_{end_date}_{request_id}"
            return FILENAME_TEMPLATE.format(
                product_id=message.product_id,
                request_id=message.request_id,
                start_date=np.datetime_as_string(
                    kube.time.values[0], unit="D"
                ),
                end_date=np.datetime_as_string(kube.time.values[-1], unit="D"),
            )
        else:
            FILENAME_TEMPLATE = (
                "VHR-PRO_IT2km_CMCC-CM_{product_id}_CCLM5-0-9_1hr_{request_id}"
            )
            return FILENAME_TEMPLATE.format(
                product_id=message.product_id,
                request_id=message.request_id,
            )


def rcp85_filename_condition(kube: DataCube, message: Message) -> bool:
    return (
        message.dataset_id == "climate-projections-rcp85-downscaled-over-italy"
    )


def get_history_message():
    return (
        f"Generated by CMCC DDS version 0.9.0 {str(datetime.datetime.now())}"
    )


def persist_datacube(
    kube: DataCube,
    message: Message,
    base_path: str | os.PathLike,
) -> str | os.PathLike:
    if rcp85_filename_condition(kube, message):
        path = get_file_name_for_climate_downscaled(kube, message)
    else:
        var_names = list(kube.fields.keys())
        if len(kube) == 1:
            path = "_".join([
                var_names[0],
                message.dataset_id,
                message.product_id,
                str(message.request_id),
            ])
        else:
            path = "_".join([
                message.dataset_id,
                message.product_id,
                str(message.request_id),
            ])
    kube._properties["history"] = get_history_message()
    if isinstance(message.content, GeoQuery):
        format = message.content.format
    else:
        format = "netcdf"
    match format:
        case "netcdf":
            full_path = os.path.join(base_path, f"{path}.nc")
            kube.to_netcdf(full_path)
        case "geojson":
            full_path = os.path.join(base_path, f"{path}.json")
            kube.to_geojson(full_path)
        case _:
            raise ValueError(f"format `{format}` is not supported")
    return full_path


def persist_dataset(
    dset: Dataset,
    message: Message,
    base_path: str | os.PathLike,
):
    def _get_attr_comb(dataframe_item, attrs):
        return "_".join([dataframe_item[attr_name] for attr_name in attrs])

    def _persist_single_datacube(dataframe_item, base_path, format):
        dcube = dataframe_item[dset.DATACUBE_COL]
        if isinstance(dcube, Delayed):
            dcube = dcube.compute()
        if len(dcube) == 0:
            return None
        for field in dcube.fields.values():
            if 0 in field.shape:
                return None
        attr_str = _get_attr_comb(dataframe_item, dset._Dataset__attrs)
        var_names = list(dcube.fields.keys())
        if len(dcube) == 1:
            path = "_".join([
                var_names[0],
                message.dataset_id,
                message.product_id,
                attr_str,
                str(message.request_id),
            ])
        else:
            path = "_".join([
                message.dataset_id,
                message.product_id,
                attr_str,
                str(message.request_id),
            ])
        match format:
            case "netcdf":
                full_path = os.path.join(base_path, f"{path}.nc")
                dcube.to_netcdf(full_path)
            case "geojson":
                full_path = os.path.join(base_path, f"{path}.json")
                dcube.to_geojson(full_path)
        return full_path

    if isinstance(message.content, GeoQuery):
        format = message.content.format
    else:
        format = "netcdf"
    datacubes_paths = dset.data.apply(
        _persist_single_datacube, base_path=base_path, format=format, axis=1
    )
    paths = datacubes_paths[~datacubes_paths.isna()]
    if len(paths) == 0:
        return None
    elif len(paths) == 1:
        return paths.iloc[0]
    zip_name = "_".join(
        [message.dataset_id, message.product_id, str(message.request_id)]
    )
    path = os.path.join(base_path, f"{zip_name}.zip")
    with ZipFile(path, "w") as archive:
        for file in paths:
            archive.write(file, arcname=os.path.basename(file))
    for file in paths:
        os.remove(file)
    return path
